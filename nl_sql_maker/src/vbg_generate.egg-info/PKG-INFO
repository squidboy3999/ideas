Metadata-Version: 2.4
Name: vbg-generate
Version: 0.1.0
Summary: A Natural Language to SQL converter with a modular pipeline and a runtime CLI.
Author-email: Your Name <your.email@example.com>
License: MIT
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: lark>=1.1.7
Requires-Dist: PyYAML>=6.0
Requires-Dist: shapely>=2.0.0
Requires-Dist: SQLAlchemy>=2.0
Requires-Dist: pysqlite3
Requires-Dist: inflect>=7.0.0
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: flake8; extra == "dev"

# VBG (vocabulary, bindings, grammar) Pipeline

---

# Phase A — Ingest & Normalize

## A1. `load_sources(schema_path, keywords_path) -> (schema_raw, keywords_raw)`

**Transform**

* Read YAML and return raw dicts; do not modify

**Validation Gate**

* Both are dicts
* Required top-level keys exist:

  * `schema_raw` has `tables` (dict)
  * `keywords_raw` has at least one of: `functions`, `operators`, `connectors`, `vocabulary`
* No YAML anchors/tuples/unsafe types

**Diagnostics captured**

* File sizes, SHA256 of raw text, loader timings

**Typical failures**

* Missing files / empty YAML
* Unexpected root types (list/str)

---

## A2. `canonicalize_identifiers(schema_raw, keywords_raw) -> (schema_norm, keywords_norm)`

**Transform**

* Lowercase canonicals; keep alias *surfaces* as-is
* Normalize SQL types into buckets
* Normalize column labels (e.g., `["id","postgis"]`)

**Validation Gate**

* All table/column canonical names are lower\_snakecase strings
* Column `type` ∈ allowed buckets; `labels` is list\[str]
* No duplicate table names; no duplicate column names *within* a table

**Diagnostics captured**

* List of columns coerced to type bucket
* Labels inferred vs. supplied

**Typical failures**

* Mixed case canonicals
* Free-form types (“VARCHAR(255)”) not bucketed

---

# Phase B — Base Graph (entities + structure)

## B1. `build_table_column_entities(schema_norm) -> graph_v1`

**Transform**

* Create `table` nodes with `metadata.columns` (names only) & `aliases` (name, singular/plural)
* Create `column` nodes with `metadata.type`, `labels`

**Validation Gate**

* `graph_v1[table_name].entity_type == "table"`
* `graph_v1[column_name].entity_type == "column"`
* `table.metadata.columns` is a dict of column canonicals
* Each column belongs to exactly one table (check ownership map)

**Diagnostics captured**

* Orphan columns, duplicate columns across tables

**Typical failures**

* Column listed in two tables
* Non-string column names

---

## B2. `build_function_operator_entities(keywords_norm) -> graph_v2`

**Transform**

* Add `sql_actions`, `postgis_actions`, `comparison_operators`, and keyword classes (`prepositions`, `logical_operators`, `select_verbs`, `filler_words`) from keywords
* Attach `metadata.aliases` and starter `binder` fields if present

**Validation Gate**

* Every function/operator has `entity_type` in known set
* If `binder` present: `returns_type` is known; `args` list is well-formed

**Diagnostics captured**

* Unknown function classes; missing returns\_type

**Typical failures**

* Functions with empty alias sets
* Operator listed under wrong class

---

## B3. `attach_connectors_catalog(keywords_norm) -> graph_v3`

**Transform**

* Create `graph.meta.connectors` with canonical → surface mapping for `OF, FROM, AND, COMMA` (+ any others)

**Validation Gate**

* Required connectors present with non-empty surfaces
* Surfaces are **not** also assigned as non-connector aliases

**Diagnostics captured**

* Connectors colliding with other alias surfaces

**Typical failures**

* `and` also used as a table alias

---

# Phase C — Enrichment & Profiling

## C1. `attach_ownership_edges(graph_v3) -> graph_v4`

**Transform**

* Add `table.columns` and `column.table` edges
* If FK hints exist, add `edges.fk: [ (sales.user_id → users.user_id) ]`

**Validation Gate**

* For every column node, `metadata.table` exists and round-trips to the parent
* FK edges reference existing nodes

**Diagnostics captured**

* Dangling FK edges; tables with zero columns

**Typical failures**

* Typo in FK table/column name

---

## C2. `infer_types_and_labels(graph_v4) -> graph_v4e`

**Transform**

* Heuristics: `*_id` → `id`; `lat/lon` variants → geo labels; geometry presence → `postgis`
* (Optional) Light DB sampling to refine `type_category`

**Validation Gate**

* No conflicting label rules (e.g., both `postgis` and `not postgis` policy markers)
* All inferred labels are from a whitelisted set

**Diagnostics captured**

* Which labels inferred vs. declared; sampling decisions

**Typical failures**

* Columns inferred as geometry but have text types

---

# Phase D — Alias System

## D1. `seed_alias_candidates(graph_v4e, keywords_norm, alias_dict) -> alias_candidates`

**Transform**

* For each canonical, collect candidates:

  * Canonical itself
  * De-underscored
  * Safe singular/plural (last-token only)
  * Merged with `keywords_norm` synonyms and persistent `alias_dict`

**Validation Gate**

* All alias surfaces are non-empty strings
* No duplicates per canonical (case-insensitive)

**Diagnostics captured**

* Source provenance per alias (manual/plural/inferred)

**Typical failures**

* Aliases equal to reserved tokens

---

## D2. `score_alias_candidates(alias_candidates) -> alias_scored`

**Transform**

* Score (alias, canonical) with token Jaccard, edit distance, and source weights

**Validation Gate**

* Scores are floats in \[0,1]
* At least one alias per canonical above a minimum threshold (e.g., 0.2) — else mark for coverage work

**Diagnostics captured**

* Bottom 5% alias pairs and why they barely passed

**Typical failures**

* Weak alias with no lexical similarity sneaks in

---

## D3. `resolve_within_table_collisions(graph_v4e, alias_scored) -> alias_table_safe`

**Transform**

* For each table: detect alias surfaces assigned to 2+ columns; either:

  * Drop on all, or
  * Keep on a single “primary” (labels: `id` > `dimension` > `measure`, then score)
* Record a **warning** entry per drop

**Validation Gate**

* Within any single table, **no alias** maps to >1 column
* Across tables: duplicates allowed (will be disambiguated by FROM/OF later)

**Diagnostics captured**

* All drops, table+columns affected, decision reason

**Typical failures**

* Shared alias `amount` applied to `price` and `balance` in `sales`

---

## D4. `promote_alias_records(alias_table_safe) -> graph_v5 (aliases section)`

**Transform**

* Turn candidates into first-class alias records:

  ```yaml
  aliases:
    - surface: "amount"
      scope: "table:sales" | "column:price" | "global"
      targets: [{canonical: "price", weight: 0.7}]
      provenance: ["manual","plural"]
  ```

**Validation Gate**

* `surface` unique within the same `scope`
* `targets` non-empty and reference existing canonicals

**Diagnostics captured**

* Surfaces that needed scope tightening

**Typical failures**

* Aliases without targets; targets that don’t exist

---

## D5. `stamp_alias_policies(graph_v5) -> graph_v5p`

**Transform**

* Apply policy markers to aliases:

  * `preposition_purity` flags for bare `of/from/in/on/at`
  * `domain_prefer_spatial` flags on mixed spatial/textual overlaps
  * `prefix_protect` marks for single tokens that prefix longer keys (table danger)
  * `reserved_token` flags

**Validation Gate**

* Bare prepositions do **not** target non-preposition canonicals
* Reserved tokens appear only in their native classes

**Diagnostics captured**

* All policy-triggered drops or demotions

**Typical failures**

* `in` surfaces targeting table `invoices`

---

# Phase E — Function Signatures

## E1. `compile_function_signatures(graph_v5p, keywords_norm) -> graph_v6`

**Transform**

* For each function, assemble binder-relevant signature:

  * `arity`, `requires_connector` (`of`), `applicable_types`, `label_rules`, `returns_type`, `class`, `arg_position_ban` (e.g., for ordering functions)

**Validation Gate**

* Arity compatible with `requires_connector` (e.g., `avg` requires args)
* `applicable_types` variables well-formed; buckets exist
* Ordering functions listed in `arg_position_ban`

**Diagnostics captured**

* Functions with underspecified types; defaults applied

**Typical failures**

* `st_buffer` missing geometry type constraints

---

# Phase F — Global Policy Blocks

## F1. `compile_global_policies() -> policies`

**Transform**

* Emit a single source-of-truth:

  * `preposition_purity`, `reserved_tokens`, `of_canonicals`, `domain_prefer_spatial`, `arg_function_ban`, `alias_entropy_caps`

**Validation Gate**

* All identifiers referenced by policies actually exist in the graph (where applicable)
* Caps are numeric; thresholds sensible

**Diagnostics captured**

* Unknown tokens referenced by policy names

**Typical failures**

* Domain preference names don’t match canonicals

---

# Phase G — Diagnostics (persisted)

## G1. `compute_graph_diagnostics(graph_v6, policies) -> graph_v7`

**Transform**

* Run graph-level checks and **persist** summaries under `_diagnostics`:

  * Alias collisions (respecting within-table rule)
  * Reserved token misuse
  * Prefix collisions
  * Vocabulary coverage gaps (alias surfaces in graph not present in vocab yet)

**Validation Gate**

* None (this step *produces* validated diagnostics rather than gates), but set severity levels that later stages can fail on (e.g., FAIL on reserved misuse)

**Diagnostics captured**

* Structured lists with counts and exemplars

**Typical failures**

* N/A — surfaced as diagnostic payloads

---

# Phase H — Artifact Compilation

## H1. `compile_vocabulary(graph_v7, policies) -> vocabulary.yaml`

**Transform**

* Deterministic: single-word, single-target; connectors; filler words → `""`
* Non-deterministic: multi-target or multi-word
* Enforce:

  * Preposition purity (bare preps → preposition canonicals only)
  * OF-policy (`avg of`, `sum of`, `st_distance of`)
  * Prefix-protection: drop single token that prefixes longer keys if it maps to tables

**Validation Gate**

* **Coverage**: every graph alias surface appears in vocab (except those *intentionally* dropped by policy; list them)
* **Partition sanity**: single-word single-meaning in deterministic; multi-word/multi-meaning in ND
* **Serialization**: YAML round-trip; types correct

**Diagnostics captured**

* Explicit list of aliases omitted by policy (so V1 reports can annotate “acceptable omission”)

**Typical failures**

* Canonicals lacking identity mapping
* Bare `in` mapping to table

---

## H2. `compile_binder_catalogs(graph_v7, policies) -> binder.yaml`

**Transform**

* Emit catalogs:

  * `functions` (signatures), `columns` (type/labels/owner), `tables`, `connectors`
* (Optional) minimal templates
* Include `_diagnostics` passthrough for debuggability

**Validation Gate**

* Cross-link checks:

  * All functions/columns/tables exist in graph and have identity in vocabulary
  * Connectors used in templates exist in vocabulary (lowercase ‘of/from/and’)
* Shape checks: all catalogs are dict/list as expected

**Diagnostics captured**

* Missing identity mappings, missing connectors

**Typical failures**

* Columns without owner table
* Functions missing from graph

---

## H3. `compile_canonical_grammar(graph_v7) -> canonical_grammar.lark`

**Transform**

* Keep grammar **purely canonical**:

  * `SELECT column_list (FROM|OF) CANONICAL_TABLE`
  * `column_list := selectable (',' selectable)* (','? 'AND' selectable)?`
  * `selectable := CANONICAL_COLUMN | CANONICAL_FUNCTION ['OF' column_list]`

**Validation Gate**

* Grammar builds (`Lark(...)`) without errors
* Minimal smoke parse for synthetic sentences derived from graph (1 table, 1 column, 1 function)

**Diagnostics captured**

* Grammar build time, nonterminals count

**Typical failures**

* Terminal conflicts if names not uppercased consistently

---

# Phase I — Cross-Artifact Sanity (no further sub-gates needed)

## I1. `validate_canonical_roundtrip(graph_v7, grammar) -> report`

* Use SmartGenerator to produce canonical phrases
* Bind using **runtime CanonicalBinder** (strict types off, coercion on for test)
* Serialize back and parse again
* **Pass/Fail threshold** (e.g., ≥95%)

## I2. `validate_binder_sql_feasibility(graph_v7) -> report`

* Sample functions, ensure at least one compatible column exists for placeholders

---

# Phase J — Integration Sanity (normalizer in the loop)

## J1. `validate_full_integration(graph_v7, vocabulary, grammar) -> report`

* De-normalize canonical → messy (with reverse alias map, connector-aware)
* Normalize (vocabulary) → candidates
* Bind (runtime CanonicalBinder, **strict types ON**)
* Parse (canonical grammar)
* Record:

  * success rate
  * candidate histogram
  * failure buckets (`normalizer_zero`, `binder_fail`, `parser_fail`)
  * flight recorder for top fails

*(This stage produces a report; no internal gates except your success threshold.)*

---

# Phase K — Caching & Provenance

## K1. `compute_stability_hashes(inputs, outputs) -> graph_v7.meta.hashes`

**Transform**

* Hash raw inputs and major outputs (`graph`, `vocab`, `binder`, `grammar`)

**Validation Gate**

* Hashes exist and are strings of expected length

**Diagnostics captured**

* Full set of hashes; versions

**Typical failures**

* None (plumbing)

---

## K2. `write_provenance(meta) -> graph_v7.meta`

**Transform**

* Stamp pipeline version, timestamps, knobs (thresholds)

**Validation Gate**

* All required meta fields present

**Diagnostics captured**

* Pipeline config snapshot

---

# Phase L — Incremental Learning (optional)

## L1. `update_alias_dictionary(graph_v7, vocabulary, feedback) -> alias_dictionary.yaml`

**Transform**

* Merge “good” new aliases with scope into persistent dictionary

**Validation Gate**

* No collisions introduced within-table
* No reserved tokens added

**Diagnostics captured**

* Newly added aliases with provenance (“observed success X times”)

**Typical failures**

* Ambiguous alias learned for two columns in the same table

---

## Shared Utilities (used across phases)

* `safe_yaml_load/dump` (tuple guard, anchor removal)
* `lower_snakecase(name)`, `deunderscore(name)`
* `pluralize_last_token(name)`, `singularize_last_token(name)`
* `tokenize(s)`, `jaccard(a,b)`, `levenshtein(a,b)`
* `is_reserved_token(s)`, `is_preposition(s)`
* `hash_bytes(text)`, `timeit` decorator for profiling
* `emit_diagnostic(stage, payload)` (append-only recorder in graph `_diagnostics`)

---

## How to wire gates

* After each function returns, **run its gate**. If it fails:

  * Log a concise FAIL banner with top offenders (IDs/examples)
  * Attach the full structured failure set to `graph._diagnostics[stage]`
  * **Stop the pipeline** unless stage is explicitly “diagnostic-only” (e.g., G1, I1, I2, J1)
* This makes every step **locally testable**: feed inputs, run transform, assert gate.
